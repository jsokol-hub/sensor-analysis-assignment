{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a8b399-50ff-4d60-bf95-c80f0bc887ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_features import process_dataset, load_and_process_sample\n",
    "from visualization import signal_viewer\n",
    "from imu_pipeline import IMUPipeline\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7165b8b-235f-4a05-85ad-038a4f3784ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_viewer(\n",
    "    data_dir=Path('data/raw/train'),\n",
    "    labels_csv=Path('data/train.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading train data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "print(\"Size of training set:\", train_df.shape)\n",
    "print(\"Few first rows of training set:\\n\", train_df.head())\n",
    "display(train_df.head())\n",
    "\n",
    "#Some basic stats\n",
    "print(\"\\nStatistics by numerical features:\")\n",
    "display(train_df.describe())\n",
    "\n",
    "#Distribution of labels\n",
    "print(\"\\nDistribution of labels:\")\n",
    "display(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features for inference set\n",
    "print(\"# Generate features for inference set...\")\n",
    "inference_df = process_dataset('inference')\n",
    "print(\"Done\")\n",
    "\n",
    "#loading test data\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "print(\"\\nSize of test dataset:\", test_df.shape)\n",
    "print(\"Size of inference datase:\", inference_df.shape)\n",
    "\n",
    "# Comparison of feature distribution\n",
    "print(\"\\nComparison of feature distribution:\")\n",
    "print(\"\\nTest dataset:\")\n",
    "display(test_df.describe())\n",
    "print(\"\\nInference dataset:\")\n",
    "display(inference_df.describe())\n",
    "\n",
    "#Comparison of avg values\n",
    "print(\"\\nComparison of features avg values:\")\n",
    "mean_comparison = pd.DataFrame({\n",
    "    'test_mean': test_df.select_dtypes(include=[np.number]).mean(),\n",
    "    'inference_mean': inference_df.select_dtypes(include=[np.number]).mean(),\n",
    "    'difference': test_df.select_dtypes(include=[np.number]).mean() - inference_df.select_dtypes(include=[np.number]).mean()\n",
    "})\n",
    "display(mean_comparison)\n",
    "\n",
    "#Adding plots\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(mean_comparison.index, mean_comparison['difference'] )\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Difference between test and inference features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if 'label' in test_df.columns:\n",
    "    print(\"\\nDistribution of labels in test dataset:\")\n",
    "    display(test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cdc41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of key features to visualize\n",
    "key_features = ['x_energy', 'y_energy', 'z_energy', 'sudden_change_score', 'x_fft_max', 'y_fft_max', 'z_fft_max']\n",
    "\n",
    "for feature in key_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(test_df[feature], bins=30, alpha=0.5, label='Test')\n",
    "    plt.hist(inference_df[feature], bins=30, alpha=0.5, label='Inference')\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Calibration status distribution\n",
    "if 'calibration_status' in inference_df.columns:\n",
    "    print(\"Calibration status in inference set:\")\n",
    "    print(inference_df['calibration_status'].value_counts())\n",
    "    inference_df['calibration_status'].value_counts().plot(kind='bar', title='Calibration Status Distribution')\n",
    "    plt.show()\n",
    "\n",
    "# Weather distribution\n",
    "if 'weather' in inference_df.columns:\n",
    "    print(\"Weather in inference set:\")\n",
    "    print(inference_df['weather'].value_counts())\n",
    "    inference_df['weather'].value_counts().plot(kind='bar', title='Weather Distribution')\n",
    "    plt.show()\n",
    "\n",
    "# Device model distribution\n",
    "if 'device_model' in inference_df.columns:\n",
    "    print(\"Device model in inference set:\")\n",
    "    print(inference_df['device_model'].value_counts())\n",
    "    inference_df['device_model'].value_counts().plot(kind='bar', title='Device Model Distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1733a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading model\n",
    "model = joblib.load(\"models/imu_pipeline.pkl\")\n",
    "\n",
    "#Loading labels for inference dataset\n",
    "inference_labels = pd.read_csv('data/manual_annotation/inference_labels.csv')\n",
    "print(\"\\nShape of inference dataset:\", inference_df.shape)\n",
    "print(\"Shape of inference labels:\", inference_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fece2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions\n",
    "predictions = model.predict(inference_df)\n",
    "probabilities = model.predict_proba(inference_df)\n",
    "\n",
    "#Joining predictions and real labels\n",
    "results_df = pd.DataFrame({\n",
    "    'sample_id': inference_df['sample_id'],\n",
    "    'predicted': predictions,\n",
    "    'actual': inference_labels['label']\n",
    "})\n",
    "\n",
    "#Accuracy calculation\n",
    "accuracy = (results_df['predicted'] == results_df['actual']).mean()\n",
    "print(f\"Accuracy in inference dataset: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaba4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error matrix\n",
    "cm = confusion_matrix(results_df[\"actual\"], results_df[\"predicted\"])\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title(\"Error matrix (inference dataset)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "#Classification report\n",
    "print(\"\\nClassification report (inference dataset):\")\n",
    "print(classification_report(results_df[\"actual\"], results_df[\"predicted\"]))\n",
    "\n",
    "\n",
    "#If test_df has labels\n",
    "if 'label' in test_df.columns:\n",
    "    test_predictions = model.predict(test_df)\n",
    "    test_accuracy = (test_predictions == test_df['label']).mean()\n",
    "    print(f\"Accuracy in test dataset: {test_accuracy:.2%}\")\n",
    "    print(\"\\nClassification report (test dataset):\")\n",
    "    print(classification_report(test_df['label'], test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319cf27-f9e4-470b-9218-8b5bd4f9d0b9",
   "metadata": {},
   "source": [
    "# ❓ Questions to Reflect On\n",
    "\n",
    "---\n",
    "\n",
    "### 1. What do you observe when comparing the model's predictions on the new data to its known performance?\n",
    "\n",
    "I observe a significant drop in model performance:\n",
    "- **On test set:** accuracy 94.50%\n",
    "- **On inference set:** accuracy 51.37%\n",
    "\n",
    "Particularly noticeable issues with the \"normal\" class:\n",
    "- **On test:** precision 0.90, recall 1.00\n",
    "- **On inference:** precision 0.54, recall 0.20\n",
    "\n",
    "This indicates that the model poorly generalizes to new data, especially in normal driving scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Is there anything in the data that might explain differences in behavior?\n",
    "\n",
    "Yes, I discovered significant differences in the data:\n",
    "1. The mean values of signal energy features (`x_energy`, `y_energy`, `z_energy`) are much higher in the inference set than in the test set. This suggests that the new data contains more intense or dynamic movements, or that the sensor characteristics or preprocessing have changed.\n",
    "2. Features related to sudden changes (`sudden_change_score`) and frequency characteristics (`fft_max`, `fft_mean`) also differ.\n",
    "3. The distribution of feature values in the inference set substantially differs from the test set.\n",
    "\n",
    "These differences explain why the model trained on one data distribution performs poorly on another.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Can you identify patterns or trends related to when the model succeeds or fails?\n",
    "\n",
    "Yes, I can identify the following patterns:\n",
    "\n",
    "**Model successes:**\n",
    "- Better at detecting collisions than normal situations.\n",
    "- Has high recall (0.83) for the collision class.\n",
    "\n",
    "**Model failures:**\n",
    "- Poor at detecting normal situations (recall 0.20).\n",
    "- Has low precision for both classes (0.51 and 0.54).\n",
    "- Frequently confuses normal situations with collisions.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Are there signals or features that seem to affect the model's reliability?\n",
    "\n",
    "Yes, I identified key features affecting model reliability:\n",
    "\n",
    "1. **Energy features:**\n",
    "   - `x_energy`, `y_energy`, `z_energy`\n",
    "   - These features show the greatest difference between datasets.\n",
    "2. **Sudden change features:**\n",
    "   - `sudden_change_score`\n",
    "   - `max_delta_mag`\n",
    "   - These features also differ significantly in new data.\n",
    "3. **Frequency characteristics:**\n",
    "   - `fft_max`\n",
    "   - `fft_mean`\n",
    "   - Show significant differences between datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. What could be done in the short term to handle the current situation?\n",
    "\n",
    "**Short-term solutions:**\n",
    "1. **Model recalibration:**\n",
    "   - Retrain the model using the inference dataset to better adapt to the new data distribution.\n",
    "   - Adjust decision thresholds for key features if retraining is not possible.\n",
    "2. **Feature adaptation:**\n",
    "   - Normalize energy-related features (such as `x_energy`, `y_energy`, `z_energy`) to reduce the impact of scale differences between datasets.\n",
    "   - Consider using relative or standardized feature values instead of absolute ones.\n",
    "3. **Regularization:**\n",
    "   - Apply L1 or L2 regularization to improve the model's robustness to feature distribution shifts.\n",
    "   - Tune the regularization parameter to achieve the best balance between bias and variance.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. What are potential long-term steps to improve model performance in similar scenarios?\n",
    "\n",
    "**Long-term solutions:**\n",
    "1. **Enhance data collection:**\n",
    "   - Gather a more diverse dataset that better represents real-world scenarios.\n",
    "   - Ensure data is collected under a variety of conditions (e.g., different weather, road types, and vehicle speeds).\n",
    "   - Increase the number of samples for each class to improve class balance and model generalization.\n",
    "2. **Improve feature engineering:**\n",
    "   - Develop more robust and informative features that are less sensitive to changes in data distribution.\n",
    "   - Utilize relative or standardized metrics where appropriate.\n",
    "   - Incorporate contextual features that capture additional information about the driving environment or situation.\n",
    "3. **Advance model development:**\n",
    "   - Explore ensemble methods to increase predictive stability and accuracy.\n",
    "   - Investigate adaptive learning techniques to help the model adjust to new data over time.\n",
    "   - Implement a continuous learning pipeline to regularly update the model as new labeled data becomes available.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. What would you want to explore further if given more time or data?\n",
    "\n",
    "**Additional research:**\n",
    "1. **Data analysis:**\n",
    "   - Study the impact of metadata (weather, road type, speed).\n",
    "   - Analyze temporal patterns.\n",
    "   - Investigate feature correlations.\n",
    "2. **Model improvement:**\n",
    "   - Try other algorithms.\n",
    "   - Research ensemble methods.\n",
    "   - Test deep learning.\n",
    "3. **Validation:**\n",
    "   - Collect more test data.\n",
    "   - Conduct A/B testing of different approaches.\n",
    "   - Evaluate performance in real-world conditions.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. What assumptions did the model rely on during training — and are they still valid?\n",
    "\n",
    "1. **Data distribution stability:**  \n",
    "   The model assumed that the feature distributions in the training and future data would be similar. In reality, there is a significant distribution shift, which leads to degraded performance.\n",
    "2. **Sensor and processing consistency:**  \n",
    "   It was assumed that devices, calibration, and data processing would remain unchanged. In practice, differences in hardware, firmware, or collection conditions can affect the data.\n",
    "3. **Labeling consistency:**  \n",
    "   The model relied on consistent class labeling rules. Any changes in annotation guidelines could negatively impact results.\n",
    "4. **Feature relevance:**  \n",
    "   It was assumed that the selected features would remain equally informative for new data. However, due to distribution shifts, their importance may have changed.\n",
    "\n",
    "**Conclusion:**  \n",
    "Most of these assumptions did not fully hold, which explains the drop in model performance on new data. The model needs to be adapted, and data collection and processing procedures should be improved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
